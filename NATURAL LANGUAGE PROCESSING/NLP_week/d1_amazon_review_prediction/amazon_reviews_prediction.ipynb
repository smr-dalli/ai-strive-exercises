{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_notebook_id": "6e5a2c14-6d2c-405d-9849-75d083f9c46b",
    "deepnote_execution_queue": [],
    "colab": {
      "name": "amazon_reviews_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00000-0fa29134-994c-426c-a338-424ff72fb27e",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b19f492f",
        "execution_millis": 9318,
        "execution_start": 1619433235434,
        "deepnote_cell_type": "code",
        "id": "ClZwWoPai_6P"
      },
      "source": [
        "# import libraries\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import torch\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00001-8aba5d8d-de3b-40ca-a4ea-9518c56e22e1",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "4e6f03dc",
        "execution_millis": 27,
        "execution_start": 1619435587593,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bMBrcqLEi_6S",
        "outputId": "b1eb704b-e0e3-4b5a-f495-8b112123cd1b"
      },
      "source": [
        "# loading train data\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/amazon_reviews_data/train.csv', header=None)\n",
        "train_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>more like funchuck</td>\n",
              "      <td>Gave this to my dad for a gag gift after direc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>Inspiring</td>\n",
              "      <td>I hope a lot of people hear this cd. We need m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>The best soundtrack ever to anything.</td>\n",
              "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Chrono Cross OST</td>\n",
              "      <td>The music of Yasunori Misuda is without questi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Too good to be true</td>\n",
              "      <td>Probably the greatest soundtrack in history! U...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  ...                                                  2\n",
              "0  3  ...  Gave this to my dad for a gag gift after direc...\n",
              "1  5  ...  I hope a lot of people hear this cd. We need m...\n",
              "2  5  ...  I'm reading a lot of reviews saying that this ...\n",
              "3  4  ...  The music of Yasunori Misuda is without questi...\n",
              "4  5  ...  Probably the greatest soundtrack in history! U...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00002-54494f21-d9e6-46c3-a3f6-e3040fe5c31f",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "5cf348cb",
        "execution_start": 1619435605104,
        "execution_millis": 21,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jqjpJrS9i_6V",
        "outputId": "59d4b53f-8130-4192-b82b-d591d46fd8cc"
      },
      "source": [
        "# loading test data\n",
        "\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/amazon_reviews_data/test.csv', header=None)\n",
        "test_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>mens ultrasheer</td>\n",
              "      <td>This model may be ok for sedentary types, but ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Surprisingly delightful</td>\n",
              "      <td>This is a fast read filled with unexpected hum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Works, but not as advertised</td>\n",
              "      <td>I bought one of these chargers..the instructio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>Oh dear</td>\n",
              "      <td>I was excited to find a book ostensibly about ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>Incorrect disc!</td>\n",
              "      <td>I am a big JVC fan, but I do not like this mod...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  ...                                                  2\n",
              "0  1  ...  This model may be ok for sedentary types, but ...\n",
              "1  4  ...  This is a fast read filled with unexpected hum...\n",
              "2  2  ...  I bought one of these chargers..the instructio...\n",
              "3  2  ...  I was excited to find a book ostensibly about ...\n",
              "4  2  ...  I am a big JVC fan, but I do not like this mod...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00004-2e049d93-f63b-42cc-824a-d334d2ddc89d",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e76e1812",
        "execution_millis": 1,
        "execution_start": 1619435662303,
        "deepnote_cell_type": "code",
        "id": "ZeNYCWMNi_6X"
      },
      "source": [
        "# rename columns\n",
        "train_df.rename({0:\"star\", 1:\"rating1\", 2:\"rating2\"}, axis = 1, inplace = True)\n",
        "\n",
        "# merge the two reviews columns\n",
        "train_df[\"review\"] = train_df[\"rating1\"] + \" \" +  train_df[\"rating2\"]\n",
        "\n",
        "# drop unnecessary columns\n",
        "train_df.drop(columns=[\"rating1\", \"rating2\"], inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfdxg8gGjDaf"
      },
      "source": [
        "# sample train_df\n",
        "\n",
        "train_df = train_df.groupby('star', group_keys=False).apply(lambda x: x.sample(1000))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00004-f11f8c0f-cd1e-47a4-93e2-31db145643b0",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "120a37af",
        "execution_start": 1619435662791,
        "execution_millis": 3,
        "deepnote_cell_type": "code",
        "id": "RA-Rd5Mzi_6Y"
      },
      "source": [
        "# rename columns\n",
        "test_df.rename({0:\"star\", 1:\"rating1\", 2:\"rating2\"}, axis = 1, inplace = True)\n",
        "\n",
        "# merge the two reviews columns\n",
        "test_df[\"review\"] = test_df[\"rating1\"] + \" \" +  test_df[\"rating2\"]\n",
        "\n",
        "# drop unnecessary columns\n",
        "test_df.drop(columns=[\"rating1\", \"rating2\"], inplace=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUGYmXJCjKth"
      },
      "source": [
        "# sample test_df\n",
        "\n",
        "test_df = test_df.groupby('star', group_keys=False).apply(lambda x: x.sample(200))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "YQMA9jKZwbz8",
        "outputId": "24a8e4a3-48b8-4d93-e80f-845e36d604ba"
      },
      "source": [
        "test_df['tmp'] = test_df['review'].apply(preprocessing).apply(len)\n",
        "test_df.describe()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>star</th>\n",
              "      <th>tmp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>35.796000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.414921</td>\n",
              "      <td>19.473253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>32.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>49.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5.000000</td>\n",
              "      <td>141.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              star          tmp\n",
              "count  1000.000000  1000.000000\n",
              "mean      3.000000    35.796000\n",
              "std       1.414921    19.473253\n",
              "min       1.000000     7.000000\n",
              "25%       2.000000    20.000000\n",
              "50%       3.000000    32.000000\n",
              "75%       4.000000    49.000000\n",
              "max       5.000000   141.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00005-14888532-284f-4972-ac4a-239e2087204a",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "7aa48487",
        "execution_millis": 5156,
        "execution_start": 1619433434549,
        "deepnote_cell_type": "code",
        "id": "atLhWocli_6Y"
      },
      "source": [
        "# load nlp model pretrained from spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00006-41c75e2d-5b5b-4f68-9edc-b2741fff458b",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "36c1a908",
        "execution_millis": 1,
        "execution_start": 1619433443637,
        "deepnote_cell_type": "code",
        "id": "ejKtTGgUi_6Z"
      },
      "source": [
        "# define preprocessing function: take the lemma of the token if the token is not punctuation or stop-word\n",
        "\n",
        "def preprocessing(sentence):\n",
        "    \"\"\"\n",
        "    params sentence: a str containing the sentence we want to preprocess\n",
        "    return the tokens list\n",
        "    \"\"\"\n",
        "    doc = nlp(sentence)\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop]\n",
        "    return tokens"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00005-22375799-3f4d-492f-8479-067a386a1085",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "641ff771",
        "execution_millis": 1,
        "execution_start": 1619434615395,
        "deepnote_cell_type": "code",
        "id": "wO3kUT80i_6a"
      },
      "source": [
        "# define custom dataloader for train data\n",
        "\n",
        "class TrainData(Dataset):\n",
        "    # initiate the class with df and maximum number of tokens as argument\n",
        "    def __init__(self, df, max_seq_len=32):\n",
        "        self.max_seq_len = max_seq_len\n",
        "        counter = Counter()    # instanziate counter\n",
        "        train_iter = iter(df.review.values)    # make the review column iterable\n",
        "        for text in train_iter:\n",
        "            counter.update(preprocessing(text))    # update the counter object with the number of words and occurrencies\n",
        "        self.vocab = Vocab(counter, min_freq=1)    # create pytorch Vocab from the counter with all the words (min_freq=1)\n",
        "        self.vocab.load_vectors(\"fasttext.simple.300d\")    # load pretrained embeddings\n",
        "        label_pipeline = lambda x: int(x) -1     # make the label range 0-4 instead of 1-5\n",
        "        self.token2idx = lambda x: self.vocab[x]  # Converts token to index\n",
        "        self.idx2token = lambda x: self.vocab.itos[int(x)]   # converts index to token\n",
        "        self.encode = lambda x:[self.token2idx(token) for token in preprocessing(x)]    # encode every token with its index\n",
        "        self.pad = lambda x: x + (max_seq_len - len(x)) * [self.token2idx(\"<pad>\")]     # add the index of \"<pad>\" as many time as needed to reach max_seq_len\n",
        "        sequences = [self.encode(sequence)[:max_seq_len] for sequence in df.review.tolist()]    # truncate the sequence if it's longer than max_seq_len\n",
        "        sequence, self.labels = zip(*[(sequence, label_pipeline(label)) for sequence, label in zip(sequences, df.star.tolist()) if sequence]) # map every sequence to its label\n",
        "        self.sequences = [self.pad(sequence) for sequence in sequences]\n",
        "\n",
        "    # mandatory methods for custom dataloader\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        assert len(self.sequences[i]) == self.max_seq_len\n",
        "        return self.sequences[i], self.labels[i]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MitnWtOrk2W1",
        "outputId": "da747618-ba75-4f96-ebfd-f16360ea9c30"
      },
      "source": [
        "# instantiate a object of TrainData with the data loaded before\n",
        "\n",
        "train_dataset = TrainData(train_df, max_seq_len=32)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/wiki.simple.vec: 293MB [00:25, 11.4MB/s]                           \n",
            "  0%|          | 0/111051 [00:00<?, ?it/s]Skipping token b'111051' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|█████████▉| 110866/111051 [00:30<00:00, 7680.68it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00008-6e76a588-2b52-4aba-b959-0d695ad93793",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b08383d9",
        "execution_start": 1619436589426,
        "execution_millis": 2,
        "deepnote_cell_type": "code",
        "id": "q_WekF-5i_6b"
      },
      "source": [
        "# define custom dataloader for test data (with the Vocab took from the train to avoid data leakages)\n",
        "\n",
        "class TestData(Dataset):\n",
        "    # initiate the class with df and maximum number of tokens as argument\n",
        "    def __init__(self, df, max_seq_len=32, vocab=train_dataset.vocab):\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.vocab = vocab\n",
        "        label_pipeline = lambda x: int(x) -1     # make the label range 0-4 instead of 1-5\n",
        "        self.token2idx = lambda x: self.vocab[x]  # Converts token to index\n",
        "        self.idx2token = lambda x: self.vocab.itos[int(x)]   # converts index to token\n",
        "        self.encode = lambda x:[self.token2idx(token) for token in preprocessing(x)]    # encode every token with its index\n",
        "        self.pad = lambda x: x + (max_seq_len - len(x)) * [self.token2idx(\"<pad>\")]     # add the index of \"<pad>\" as many time as needed to reach max_seq_len\n",
        "        sequences = [self.encode(sequence)[:max_seq_len] for sequence in df.review.tolist()]    # truncate the sequence if it's longer than max_seq_len\n",
        "        sequence, self.labels = zip(*[(sequence, label_pipeline(label)) for sequence, label in zip(sequences, df.star.tolist()) if sequence]) # map every sequence to its label\n",
        "        self.sequences = [self.pad(sequence) for sequence in sequences]\n",
        "\n",
        "    # mandatory methods for custom dataloader\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        assert len(self.sequences[i]) == self.max_seq_len\n",
        "        return self.sequences[i], self.labels[i]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OquR3-BUk9AV"
      },
      "source": [
        "# instantiate a object of TestData with the data loaded before\n",
        "\n",
        "test_dataset = TestData(test_df, max_seq_len=32)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00008-b7974cc2-ab41-471a-a4aa-8b066d4c9f11",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b39ebb95",
        "execution_millis": 1,
        "execution_start": 1619435779199,
        "deepnote_cell_type": "code",
        "id": "dzQxIKbri_6c"
      },
      "source": [
        "# define collate function to create the batch (convert the sentences to tensor)\n",
        "\n",
        "def collate(batch, vectorizer=train_dataset.vocab.vectors):\n",
        "    # stacking the tokens, transforming them into vector with the vectorizer then staking them again to get the actual batch\n",
        "    inputs = torch.stack([torch.stack([vectorizer[token] for token in sentence[0]]) for sentence in batch])\n",
        "\n",
        "    # creating a tensor for the target\n",
        "    target = torch.LongTensor([item[1] for item in batch])\n",
        "    return inputs, target"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00010-966aabc7-f33b-4a3c-a056-0b4d8274a338",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "c8006fe1",
        "execution_millis": 1,
        "execution_start": 1619436600658,
        "deepnote_cell_type": "code",
        "id": "EZzeNADXi_6e"
      },
      "source": [
        "# define batch_size and create a dataloader from the dataset\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate)\n",
        "dataloaders = {\"train\": train_loader, \"val\": test_loader}\n",
        "dataset_sizes = {\"train\": len(train_df), \"val\": len(test_df)}\n"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00011-efb6e3c7-ffd4-4dc6-8f2f-fc239a8e1da6",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "3a69c1e2",
        "execution_millis": 2,
        "execution_start": 1619436001969,
        "deepnote_cell_type": "code",
        "id": "CeNqaplyi_6e"
      },
      "source": [
        "## Created a class with layers and activation functions to train Neural Networks\n",
        "max_seq_len = 32\n",
        "emb_dim = 300\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, max_seq_len, emb_dim, hidden1=256, hidden2=128):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(max_seq_len * emb_dim, hidden1)\n",
        "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "        self.fc3 = nn.Linear(hidden2, 5)\n",
        "        self.out = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = F.relu(self.fc1(inputs.squeeze(1).float()))\n",
        "        x = self.fc2(x)\n",
        "        x =  self.fc3(x)\n",
        "\n",
        "        return self.out(x)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00012-8635750e-f176-4e81-a8e2-ccf2276a9a9b",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "4b5fb337",
        "execution_millis": 15,
        "execution_start": 1619435827875,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk8jrst0i_6f",
        "outputId": "95662fb3-bb11-4a5c-d7e4-2ce45ec2a658"
      },
      "source": [
        "model = Classifier(32, 300, 256, 128) # 32 = max_seq_leng, 300 = dim of vectors\n",
        "model"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (fc1): Linear(in_features=9600, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=5, bias=True)\n",
              "  (out): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00013-1f052170-34f8-4488-b885-df687f8a9a3b",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "80117d85",
        "execution_millis": 0,
        "execution_start": 1619435829130,
        "deepnote_cell_type": "code",
        "id": "83eaG6lhi_6g"
      },
      "source": [
        "# NLLLoss for log_soft_max activation and Adam optimizer\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00014-87c46d4d-8121-49ee-b6e1-68f9ea723fd6",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "eca8ec9",
        "execution_start": 1619434734443,
        "execution_millis": 11,
        "deepnote_cell_type": "code",
        "id": "RUlAw8Dbi_6g"
      },
      "source": [
        "\"\"\"\n",
        "dataiter = iter(train_loader)\n",
        "sentences, labels = dataiter.next()\n",
        "sentences.resize_(16, 1, 32*emb_dim).shape\n",
        "sentence_idx = 0\n",
        "log_ps = model.forward(sentences[sentence_idx, :])\n",
        "sentence = sentences[sentence_idx]\n",
        "torch.exp(log_ps)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00015-4dac61ce-1add-49d9-af58-483f36d5bcfe",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "6bad3d92",
        "execution_start": 1619435876142,
        "execution_millis": 2,
        "deepnote_cell_type": "code",
        "id": "2ZAXNQgJi_6g"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtxsKo4UpWfe",
        "outputId": "0a71e296-de1d-47bb-f1d5-755faa38814e"
      },
      "source": [
        "device"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00018-0d2bc47d-35de-494b-ae4e-2dfe828deaa5",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "abd57ea1",
        "execution_millis": 285,
        "execution_start": 1619436608797,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRcBaoIDi_6g",
        "outputId": "9cb6cf1b-0672-494b-d5b6-ced92c98e85a"
      },
      "source": [
        "# training loop\n",
        "emb_dim = 300\n",
        "epochs = 10\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "print_every = 50\n",
        "max_accuracy = 0\n",
        "model = model.to(device)\n",
        "for epoch in range(epochs):\n",
        "    for sentences, labels in train_loader:\n",
        "        sentences.resize_(sentences.size()[0], 32*emb_dim)\n",
        "        #sentences.resize_(sentences.size()[0], 1, max_seq_len * emb_dim) # resize the vector to be of [batch size, max_seq_len * embedding_dimension] (fc layer input)\n",
        "\n",
        "        steps += 1\n",
        "        # Move input and label tensors to the default device\n",
        "        sentences, labels = sentences.to(device), labels.to(device)\n",
        "        \n",
        "        ## bringing the weights to zero \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        log_ps = model.forward(sentences)\n",
        "        loss = criterion(log_ps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if steps % print_every == 0:\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for sentences, labels in test_loader:\n",
        "                    sentences.resize_(sentences.size()[0], 1, 32* emb_dim)\n",
        "                    sentences, labels = sentences.to(device), labels.to(device)\n",
        "                    log_ps = model.forward(sentences)\n",
        "                    batch_loss = criterion(log_ps, labels)\n",
        "                    \n",
        "                    test_loss += batch_loss.item()\n",
        "                    \n",
        "                    # Calculate accuracy\n",
        "                    ps = torch.exp(log_ps)\n",
        "                    top_p, top_class = ps.topk(1, dim=1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "                    if accuracy >= max_accuracy:\n",
        "                        max_accuracy = accuracy\n",
        "                        torch.save(model.state_dict(), 'checkpoint.pth')\n",
        "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
        "                  f\"Test loss: {test_loss/len(test_loader):.3f}.. \"\n",
        "                  f\"Test accuracy: {accuracy/len(test_loader):.3f}\")\n",
        "            running_loss = 0\n",
        "            model.train()"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10.. Train loss: 0.055.. Test loss: 27.616.. Test accuracy: 0.282\n",
            "Epoch 1/10.. Train loss: 0.038.. Test loss: 26.142.. Test accuracy: 0.285\n",
            "Epoch 1/10.. Train loss: 0.002.. Test loss: 25.273.. Test accuracy: 0.294\n",
            "Epoch 1/10.. Train loss: 0.019.. Test loss: 25.660.. Test accuracy: 0.300\n",
            "Epoch 1/10.. Train loss: 0.030.. Test loss: 26.888.. Test accuracy: 0.294\n",
            "Epoch 1/10.. Train loss: 0.100.. Test loss: 28.030.. Test accuracy: 0.275\n",
            "Epoch 2/10.. Train loss: 0.019.. Test loss: 28.541.. Test accuracy: 0.279\n",
            "Epoch 2/10.. Train loss: 0.030.. Test loss: 29.374.. Test accuracy: 0.284\n",
            "Epoch 2/10.. Train loss: 0.003.. Test loss: 27.727.. Test accuracy: 0.290\n",
            "Epoch 2/10.. Train loss: 0.080.. Test loss: 27.105.. Test accuracy: 0.280\n",
            "Epoch 2/10.. Train loss: 0.126.. Test loss: 26.783.. Test accuracy: 0.285\n",
            "Epoch 2/10.. Train loss: 0.022.. Test loss: 27.000.. Test accuracy: 0.281\n",
            "Epoch 3/10.. Train loss: 0.024.. Test loss: 27.492.. Test accuracy: 0.291\n",
            "Epoch 3/10.. Train loss: 0.004.. Test loss: 27.534.. Test accuracy: 0.293\n",
            "Epoch 3/10.. Train loss: 0.012.. Test loss: 28.436.. Test accuracy: 0.288\n",
            "Epoch 3/10.. Train loss: 0.006.. Test loss: 28.613.. Test accuracy: 0.281\n",
            "Epoch 3/10.. Train loss: 0.040.. Test loss: 30.210.. Test accuracy: 0.292\n",
            "Epoch 3/10.. Train loss: 0.076.. Test loss: 27.817.. Test accuracy: 0.300\n",
            "Epoch 4/10.. Train loss: 0.097.. Test loss: 24.517.. Test accuracy: 0.283\n",
            "Epoch 4/10.. Train loss: 0.005.. Test loss: 24.520.. Test accuracy: 0.271\n",
            "Epoch 4/10.. Train loss: 0.018.. Test loss: 25.393.. Test accuracy: 0.272\n",
            "Epoch 4/10.. Train loss: 0.031.. Test loss: 26.470.. Test accuracy: 0.289\n",
            "Epoch 4/10.. Train loss: 0.139.. Test loss: 22.928.. Test accuracy: 0.282\n",
            "Epoch 4/10.. Train loss: 0.010.. Test loss: 23.308.. Test accuracy: 0.280\n",
            "Epoch 4/10.. Train loss: 0.030.. Test loss: 23.439.. Test accuracy: 0.285\n",
            "Epoch 5/10.. Train loss: 0.011.. Test loss: 25.364.. Test accuracy: 0.272\n",
            "Epoch 5/10.. Train loss: 0.096.. Test loss: 25.245.. Test accuracy: 0.275\n",
            "Epoch 5/10.. Train loss: 0.015.. Test loss: 25.490.. Test accuracy: 0.291\n",
            "Epoch 5/10.. Train loss: 0.015.. Test loss: 25.384.. Test accuracy: 0.284\n",
            "Epoch 5/10.. Train loss: 0.019.. Test loss: 25.502.. Test accuracy: 0.282\n",
            "Epoch 5/10.. Train loss: 0.021.. Test loss: 25.872.. Test accuracy: 0.274\n",
            "Epoch 6/10.. Train loss: 0.040.. Test loss: 24.816.. Test accuracy: 0.284\n",
            "Epoch 6/10.. Train loss: 0.027.. Test loss: 26.329.. Test accuracy: 0.272\n",
            "Epoch 6/10.. Train loss: 0.075.. Test loss: 25.291.. Test accuracy: 0.286\n",
            "Epoch 6/10.. Train loss: 0.004.. Test loss: 26.539.. Test accuracy: 0.285\n",
            "Epoch 6/10.. Train loss: 0.016.. Test loss: 26.251.. Test accuracy: 0.285\n",
            "Epoch 6/10.. Train loss: 0.041.. Test loss: 25.919.. Test accuracy: 0.291\n",
            "Epoch 7/10.. Train loss: 0.036.. Test loss: 24.562.. Test accuracy: 0.293\n",
            "Epoch 7/10.. Train loss: 0.023.. Test loss: 23.900.. Test accuracy: 0.285\n",
            "Epoch 7/10.. Train loss: 0.087.. Test loss: 22.116.. Test accuracy: 0.294\n",
            "Epoch 7/10.. Train loss: 0.035.. Test loss: 23.543.. Test accuracy: 0.304\n",
            "Epoch 7/10.. Train loss: 0.053.. Test loss: 22.395.. Test accuracy: 0.304\n",
            "Epoch 7/10.. Train loss: 0.005.. Test loss: 22.350.. Test accuracy: 0.303\n",
            "Epoch 8/10.. Train loss: 0.001.. Test loss: 22.405.. Test accuracy: 0.299\n",
            "Epoch 8/10.. Train loss: 0.003.. Test loss: 22.423.. Test accuracy: 0.299\n",
            "Epoch 8/10.. Train loss: 0.001.. Test loss: 22.461.. Test accuracy: 0.299\n",
            "Epoch 8/10.. Train loss: 0.010.. Test loss: 22.438.. Test accuracy: 0.294\n",
            "Epoch 8/10.. Train loss: 0.008.. Test loss: 22.637.. Test accuracy: 0.288\n",
            "Epoch 8/10.. Train loss: 0.005.. Test loss: 22.688.. Test accuracy: 0.289\n",
            "Epoch 8/10.. Train loss: 0.028.. Test loss: 24.023.. Test accuracy: 0.284\n",
            "Epoch 9/10.. Train loss: 0.007.. Test loss: 24.232.. Test accuracy: 0.283\n",
            "Epoch 9/10.. Train loss: 0.005.. Test loss: 22.887.. Test accuracy: 0.287\n",
            "Epoch 9/10.. Train loss: 0.027.. Test loss: 22.565.. Test accuracy: 0.306\n",
            "Epoch 9/10.. Train loss: 0.139.. Test loss: 24.928.. Test accuracy: 0.295\n",
            "Epoch 9/10.. Train loss: 0.116.. Test loss: 28.994.. Test accuracy: 0.280\n",
            "Epoch 9/10.. Train loss: 0.292.. Test loss: 27.311.. Test accuracy: 0.280\n",
            "Epoch 10/10.. Train loss: 0.006.. Test loss: 26.695.. Test accuracy: 0.283\n",
            "Epoch 10/10.. Train loss: 0.091.. Test loss: 27.823.. Test accuracy: 0.270\n",
            "Epoch 10/10.. Train loss: 0.062.. Test loss: 26.824.. Test accuracy: 0.274\n",
            "Epoch 10/10.. Train loss: 0.204.. Test loss: 30.483.. Test accuracy: 0.276\n",
            "Epoch 10/10.. Train loss: 0.168.. Test loss: 28.663.. Test accuracy: 0.286\n",
            "Epoch 10/10.. Train loss: 0.121.. Test loss: 27.547.. Test accuracy: 0.302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1CMVUx28WL0"
      },
      "source": [
        "def train_model(model, criterion, optimizer, dataloaders, emb_dim, epochs=15):\n",
        "    since = time.time()\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for e in range(0,epochs):\n",
        "        print(\"_____Epoch {}/{} ____\".format(e+1, epochs))\n",
        "        \n",
        "        for phase in [\"train\",\"val\"]:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "            \n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for sentences, labels in dataloaders[phase]:\n",
        "                # sentences.resize_(sentences.shape[0], 1,  32*emb_dim)\n",
        "                sentences.resize_(sentences.size()[0], 32*emb_dim)\n",
        "                sentences, labels = sentences.to(device), labels.to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(sentences)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * sentences.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            #if phase == 'train':\n",
        "                #scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))  "
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzDiCVsQ8bnY",
        "outputId": "aaabe07d-7097-4f6c-b345-c7e5e0b290fd"
      },
      "source": [
        "import time\n",
        "train_model(model, criterion, optimizer, dataloaders,emb_dim=300, epochs=10)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_____Epoch 1/10 ____\n",
            "train Loss: 0.1288 Acc: 0.9854\n",
            "\n",
            "val Loss: 24.5293 Acc: 0.2860\n",
            "\n",
            "_____Epoch 2/10 ____\n",
            "train Loss: 0.0809 Acc: 0.9922\n",
            "\n",
            "val Loss: 25.0115 Acc: 0.3090\n",
            "\n",
            "_____Epoch 3/10 ____\n",
            "train Loss: 0.1264 Acc: 0.9872\n",
            "\n",
            "val Loss: 25.3238 Acc: 0.3010\n",
            "\n",
            "_____Epoch 4/10 ____\n",
            "train Loss: 0.0799 Acc: 0.9918\n",
            "\n",
            "val Loss: 26.9982 Acc: 0.2910\n",
            "\n",
            "_____Epoch 5/10 ____\n",
            "train Loss: 0.0437 Acc: 0.9938\n",
            "\n",
            "val Loss: 27.2009 Acc: 0.3100\n",
            "\n",
            "_____Epoch 6/10 ____\n",
            "train Loss: 0.0370 Acc: 0.9930\n",
            "\n",
            "val Loss: 24.4489 Acc: 0.2920\n",
            "\n",
            "_____Epoch 7/10 ____\n",
            "train Loss: 0.0539 Acc: 0.9958\n",
            "\n",
            "val Loss: 27.0281 Acc: 0.2830\n",
            "\n",
            "_____Epoch 8/10 ____\n",
            "train Loss: 0.0958 Acc: 0.9888\n",
            "\n",
            "val Loss: 32.3303 Acc: 0.2930\n",
            "\n",
            "_____Epoch 9/10 ____\n",
            "train Loss: 0.1615 Acc: 0.9848\n",
            "\n",
            "val Loss: 26.2909 Acc: 0.2860\n",
            "\n",
            "_____Epoch 10/10 ____\n",
            "train Loss: 0.1352 Acc: 0.9902\n",
            "\n",
            "val Loss: 26.6597 Acc: 0.2970\n",
            "\n",
            "Training complete in 0m 16s\n",
            "Best val Acc: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYBFKqn7jqOO"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "FY0BtbBxi_6i"
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d7499417-dbdf-4925-a286-60df25457ef5' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ]
}
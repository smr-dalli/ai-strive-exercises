{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "valid-consultation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T19:32:23.755820Z",
     "start_time": "2021-03-30T19:32:22.644310Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-mortgage",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "**Exercise 1.1**\n",
    "\n",
    "You are given the words \"playing\", \"played\", \"play\". Find the lemma using spaCy for all of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "english-plate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T19:34:20.464484Z",
     "start_time": "2021-03-30T19:34:20.425793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemma of the word 'playing' is play\n",
      "The lemma of the word 'played' is play\n",
      "The lemma of the word 'playing' is play\n"
     ]
    }
   ],
   "source": [
    "words = [\"playing\", \"played\", \"playing\"]\n",
    "\n",
    "for word in words:\n",
    "    token = nlp(word)[0]\n",
    "    print(f\"The lemma of the word '{word}' is {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-commonwealth",
   "metadata": {},
   "source": [
    "**Exercise 1.2**\n",
    "\n",
    "Assign the spaCy lemmatizer to a variable instead of using the whole `nlp` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nlp.vocab.morphology.lemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-champion",
   "metadata": {},
   "source": [
    "**Exercise 1.3**\n",
    "\n",
    "Find the verb, the noun and the adjective forms of the words: \"playing\", \"played\", \"surfing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "elementary-blood",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T19:41:42.605031Z",
     "start_time": "2021-03-30T19:41:42.595092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play']\n",
      "['playing']\n",
      "['playing']\n",
      "-----------\n",
      "['play']\n",
      "['played']\n",
      "['played']\n",
      "-----------\n",
      "['surf']\n",
      "['surfing']\n",
      "['surfing']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
    "\n",
    "lemmatizer = nlp.vocab.morphology.lemmatizer\n",
    "print(lemmatizer(\"playing\", VERB))\n",
    "print(lemmatizer(\"playing\", NOUN))\n",
    "print(lemmatizer(\"playing\", ADJ))\n",
    "print(\"-----------\")\n",
    "print(lemmatizer(\"played\", VERB))\n",
    "print(lemmatizer(\"played\", NOUN))\n",
    "print(lemmatizer(\"played\", ADJ))\n",
    "print(\"-----------\")\n",
    "print(lemmatizer(\"surfing\", VERB))\n",
    "print(lemmatizer(\"surfing\", NOUN))\n",
    "print(lemmatizer(\"surfing\", ADJ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-heart",
   "metadata": {},
   "source": [
    "## Spell Checker\n",
    "\n",
    "**Exercise 1.4**\n",
    "\n",
    "In the following sentences there are some mispelling errors. Can you preprocess them to get rid of them? \n",
    "\n",
    "*N.B. there's no perfect spell-checker. Don't waste time on this. But be aware that it can be useful sometimes.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "caroline-agency",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T20:10:59.239954Z",
     "start_time": "2021-03-30T20:10:59.087475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i really like this exercise\n",
      "tis sentences are surely writer by an italian\n",
      "lets fix thissssss\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"i realy like this exerxise\", \n",
    "             \"tis sentences are surely writen by an italian\",\n",
    "             \"lets fix thissssss\"\n",
    "            ]\n",
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller()\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(spell(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-imperial",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "\n",
    "**Exercise 1.5 (★☆☆)**\n",
    "\n",
    "The training of TAs to survive the first two weeks at Strive School consists of the following sets:\n",
    "\n",
    "- 1000 reps of \"Did you google it?\"\n",
    "- 1000 reps of \"Did you search it on Google already?\"\n",
    "\n",
    "Use spaCy to explain the difference of the word \"google\" in the two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "respiratory-priest",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:57:52.063503Z",
     "start_time": "2021-03-31T04:57:51.819028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google       VERB       VB       verb, base form\n",
      "Google       PROPN      NNP      noun, proper singular\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Did you google it?\",\n",
    "             \"Did you search it on Google already?\"]\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = nlp(sentence)\n",
    "    for token in sentence:\n",
    "        if \"google\" in token.text.lower():\n",
    "            print(f'{token.text:{12}} {token.pos_:{10}} {token.tag_:{8}} {spacy.explain(token.tag_)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-iceland",
   "metadata": {},
   "source": [
    "**Exercise 1.5 (★★☆)**\n",
    "\n",
    "Get the frequencies of the POS tags in the example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "rational-xerox",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:17:13.245020Z",
     "start_time": "2021-03-31T05:17:13.220428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DET 3\n",
      "AUX 1\n",
      "NOUN 3\n",
      "PUNCT 2\n",
      "SPACE 1\n",
      "PROPN 2\n",
      "ADP 1\n",
      "PRON 1\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"This is an example sentence.\n",
    "            Count the POS tags in it.\"\"\"\n",
    "\n",
    "sentence = nlp(sentence)\n",
    "num_pos = sentence.count_by(spacy.attrs.POS)\n",
    "for k, v in num_pos.items():\n",
    "    print(sentence.vocab[k].text, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-label",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:09:39.399233Z",
     "start_time": "2021-03-31T05:09:39.364673Z"
    }
   },
   "source": [
    "**Exercise 1.7 (★★★)**\n",
    "\n",
    "(This exercises requires many steps in Pandas, no unique solution. You can hard code the name of the columns for this example if you get stuck.)\n",
    "\n",
    "Loading 10 tweets from the twitter datasets, create a dataframe containing the frequencies of each POS per tweet (see example).\n",
    "\n",
    "N.B. The column names must be the tags not the indices of the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fifth-dispute",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:16:19.616337Z",
     "start_time": "2021-03-31T06:16:19.498649Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/stock_data.csv\")\n",
    "\n",
    "df = df.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "outstanding-commonwealth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:16:19.660774Z",
     "start_time": "2021-03-31T06:16:19.656353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PGNX  Over 3.04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAP - user if so then the current downtrend wi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Monday's relative weakness. NYX WIN TIE TAP IC...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GOOG - ower trend line channel test &amp; volume s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAP will watch tomorrow for ONG entry.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1\n",
       "1  user: AAP MOVIE. 55% return for the FEA/GEED i...          1\n",
       "2  user I'd be afraid to short AMZN - they are lo...          1\n",
       "3                                  MNTA Over 12.00            1\n",
       "4                                   OI  Over 21.37            1\n",
       "5                                  PGNX  Over 3.04            1\n",
       "6  AAP - user if so then the current downtrend wi...         -1\n",
       "7  Monday's relative weakness. NYX WIN TIE TAP IC...         -1\n",
       "8  GOOG - ower trend line channel test & volume s...          1\n",
       "9             AAP will watch tomorrow for ONG entry.          1"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "median-discrimination",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:26:12.875138Z",
     "start_time": "2021-03-31T05:26:12.865565Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pos_and_frequency(sentence, tagger=nlp):\n",
    "    sentence = tagger(sentence)\n",
    "    num_pos = sentence.count_by(spacy.attrs.POS)\n",
    "    named_pos = {}\n",
    "    for k, v in num_pos.items():\n",
    "        named_pos[sentence.vocab[k].text] = v\n",
    "    return named_pos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "juvenile-paris",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:34:40.175145Z",
     "start_time": "2021-03-31T05:34:39.994609Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = []\n",
    "for i, row in df.iterrows():\n",
    "    pos_dict = get_pos_and_frequency(row.Text)\n",
    "    for key in pos_dict.keys():\n",
    "        if key not in columns:\n",
    "            columns.append(key)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "tropical-detective",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:34:45.822225Z",
     "start_time": "2021-03-31T05:34:45.814005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'PROPN',\n",
       " 'SPACE',\n",
       " 'NUM',\n",
       " 'CCONJ',\n",
       " 'PUNCT',\n",
       " 'VERB',\n",
       " 'SYM',\n",
       " 'ADV',\n",
       " 'PRON',\n",
       " 'AUX',\n",
       " 'ADJ',\n",
       " 'PART',\n",
       " 'SCONJ']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "collective-missouri",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:50:57.001480Z",
     "start_time": "2021-03-31T05:50:56.992453Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "array = np.zeros((10,len(columns)+1))\n",
    "data = pd.DataFrame(array, columns=[\"text\", *columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "sublime-trade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:50:57.097425Z",
     "start_time": "2021-03-31T05:50:57.080979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADP</th>\n",
       "      <th>DET</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>SPACE</th>\n",
       "      <th>NUM</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>VERB</th>\n",
       "      <th>SYM</th>\n",
       "      <th>ADV</th>\n",
       "      <th>PRON</th>\n",
       "      <th>AUX</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>PART</th>\n",
       "      <th>SCONJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text  NOUN  ADP  DET  PROPN  SPACE  NUM  CCONJ  PUNCT  VERB  SYM  ADV  \\\n",
       "0   0.0   0.0  0.0  0.0    0.0    0.0  0.0    0.0    0.0   0.0  0.0  0.0   \n",
       "1   0.0   0.0  0.0  0.0    0.0    0.0  0.0    0.0    0.0   0.0  0.0  0.0   \n",
       "2   0.0   0.0  0.0  0.0    0.0    0.0  0.0    0.0    0.0   0.0  0.0  0.0   \n",
       "3   0.0   0.0  0.0  0.0    0.0    0.0  0.0    0.0    0.0   0.0  0.0  0.0   \n",
       "4   0.0   0.0  0.0  0.0    0.0    0.0  0.0    0.0    0.0   0.0  0.0  0.0   \n",
       "5   0.0   0.0  0.0  0.0    0.0    0.0  0.0    0.0    0.0   0.0  0.0  0.0   \n",
       "6   0.0   0.0  0.0  0.0    0.0    0.0  0.0    0.0    0.0   0.0  0.0  0.0   \n",
       "7   0.0   0.0  0.0  0.0    0.0    0.0  0.0    0.0    0.0   0.0  0.0  0.0   \n",
       "8   0.0   0.0  0.0  0.0    0.0    0.0  0.0    0.0    0.0   0.0  0.0  0.0   \n",
       "9   0.0   0.0  0.0  0.0    0.0    0.0  0.0    0.0    0.0   0.0  0.0  0.0   \n",
       "\n",
       "   PRON  AUX  ADJ  PART  SCONJ  \n",
       "0   0.0  0.0  0.0   0.0    0.0  \n",
       "1   0.0  0.0  0.0   0.0    0.0  \n",
       "2   0.0  0.0  0.0   0.0    0.0  \n",
       "3   0.0  0.0  0.0   0.0    0.0  \n",
       "4   0.0  0.0  0.0   0.0    0.0  \n",
       "5   0.0  0.0  0.0   0.0    0.0  \n",
       "6   0.0  0.0  0.0   0.0    0.0  \n",
       "7   0.0  0.0  0.0   0.0    0.0  \n",
       "8   0.0  0.0  0.0   0.0    0.0  \n",
       "9   0.0  0.0  0.0   0.0    0.0  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "designed-mobile",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:50:57.313768Z",
     "start_time": "2021-03-31T05:50:57.211856Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    pos_dict = get_pos_and_frequency(row.Text)\n",
    "    for key, value in pos_dict.items():\n",
    "        data.loc[i, key] = value\n",
    "    data.loc[i, \"text\"] = row.Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "rural-ceremony",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:50:57.385440Z",
     "start_time": "2021-03-31T05:50:57.369822Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADP</th>\n",
       "      <th>DET</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>SPACE</th>\n",
       "      <th>NUM</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>VERB</th>\n",
       "      <th>SYM</th>\n",
       "      <th>ADV</th>\n",
       "      <th>PRON</th>\n",
       "      <th>AUX</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>PART</th>\n",
       "      <th>SCONJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PGNX  Over 3.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAP - user if so then the current downtrend wi...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Monday's relative weakness. NYX WIN TIE TAP IC...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GOOG - ower trend line channel test &amp; volume s...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAP will watch tomorrow for ONG entry.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  NOUN  ADP  DET  PROPN  \\\n",
       "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...   7.0  1.0  1.0    7.0   \n",
       "1  user: AAP MOVIE. 55% return for the FEA/GEED i...   6.0  2.0  2.0    5.0   \n",
       "2  user I'd be afraid to short AMZN - they are lo...   4.0  1.0  2.0    2.0   \n",
       "3                                  MNTA Over 12.00     0.0  1.0  0.0    1.0   \n",
       "4                                   OI  Over 21.37     0.0  1.0  0.0    1.0   \n",
       "5                                  PGNX  Over 3.04     0.0  1.0  0.0    1.0   \n",
       "6  AAP - user if so then the current downtrend wi...   7.0  1.0  2.0    0.0   \n",
       "7  Monday's relative weakness. NYX WIN TIE TAP IC...   3.0  0.0  0.0   10.0   \n",
       "8  GOOG - ower trend line channel test & volume s...   6.0  0.0  0.0    2.0   \n",
       "9             AAP will watch tomorrow for ONG entry.   2.0  1.0  0.0    2.0   \n",
       "\n",
       "   SPACE  NUM  CCONJ  PUNCT  VERB  SYM  ADV  PRON  AUX  ADJ  PART  SCONJ  \n",
       "0    1.0  2.0    1.0    1.0   1.0  0.0  0.0   0.0  0.0  0.0   0.0    0.0  \n",
       "1    2.0  2.0    0.0    4.0   0.0  1.0  1.0   0.0  0.0  0.0   0.0    0.0  \n",
       "2    0.0  0.0    1.0    5.0   2.0  0.0  0.0   2.0  2.0  3.0   1.0    2.0  \n",
       "3    1.0  1.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0    0.0  \n",
       "4    2.0  1.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0    0.0  \n",
       "5    2.0  1.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0    0.0  \n",
       "6    0.0  0.0    0.0    5.0   2.0  0.0  4.0   0.0  0.0  3.0   0.0    1.0  \n",
       "7    1.0  0.0    0.0    1.0   0.0  0.0  0.0   0.0  0.0  1.0   1.0    0.0  \n",
       "8    1.0  0.0    1.0    2.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0    0.0  \n",
       "9    0.0  0.0    0.0    1.0   2.0  0.0  0.0   0.0  0.0  0.0   0.0    0.0  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-allowance",
   "metadata": {},
   "source": [
    "## Named Entities Recognition\n",
    "\n",
    "**Exercise 1.8**\n",
    "\n",
    "In the Twitter dataset sample above, count how many entities are in each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "smooth-chrome",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:19:13.644167Z",
     "start_time": "2021-03-31T06:19:13.559490Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_ents(sentence, ner=nlp):\n",
    "    sentence = ner(sentence)\n",
    "    return len(sentence.ents)\n",
    "\n",
    "df[\"ents\"] = df.Text.apply(count_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "noticed-honor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:18:11.584150Z",
     "start_time": "2021-03-31T06:18:11.571558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PGNX  Over 3.04</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAP - user if so then the current downtrend wi...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Monday's relative weakness. NYX WIN TIE TAP IC...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GOOG - ower trend line channel test &amp; volume s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAP will watch tomorrow for ONG entry.</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment  ents\n",
       "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1     3\n",
       "1  user: AAP MOVIE. 55% return for the FEA/GEED i...          1     5\n",
       "2  user I'd be afraid to short AMZN - they are lo...          1     1\n",
       "3                                  MNTA Over 12.00            1     1\n",
       "4                                   OI  Over 21.37            1     1\n",
       "5                                  PGNX  Over 3.04            1     2\n",
       "6  AAP - user if so then the current downtrend wi...         -1     1\n",
       "7  Monday's relative weakness. NYX WIN TIE TAP IC...         -1     3\n",
       "8  GOOG - ower trend line channel test & volume s...          1     1\n",
       "9             AAP will watch tomorrow for ONG entry.          1     3"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-method",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:18:48.877669Z",
     "start_time": "2021-03-31T06:18:48.859874Z"
    }
   },
   "source": [
    "**Exercise 1.9**\n",
    "\n",
    "In the Twitter dataset sample above, create an extra column with the name of the Organization entities in the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ceramic-accordance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:20:28.240476Z",
     "start_time": "2021-03-31T06:20:28.227574Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ents(sentence, ner=nlp):\n",
    "    sentence = ner(sentence)\n",
    "    return sentence.ents\n",
    "\n",
    "def get_orgs(sentence, ner=nlp):\n",
    "    ents = get_ents(sentence, ner=ner)\n",
    "    return [ent.text for ent in ents if ent.label_==\"ORG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cubic-favor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:20:50.058745Z",
     "start_time": "2021-03-31T06:20:49.970719Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Organizations\"] = df.Text.apply(get_orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "illegal-monthly",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:20:51.522613Z",
     "start_time": "2021-03-31T06:20:51.500337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ents</th>\n",
       "      <th>Organizations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[AAP MOVIE, FEA/GEED]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[eBooks]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PGNX  Over 3.04</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[PGNX]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAP - user if so then the current downtrend wi...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>[AAP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Monday's relative weakness. NYX WIN TIE TAP IC...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>[NYX]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GOOG - ower trend line channel test &amp; volume s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[GOOG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAP will watch tomorrow for ONG entry.</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[AAP, ONG]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment  ents  \\\n",
       "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1     3   \n",
       "1  user: AAP MOVIE. 55% return for the FEA/GEED i...          1     5   \n",
       "2  user I'd be afraid to short AMZN - they are lo...          1     1   \n",
       "3                                  MNTA Over 12.00            1     1   \n",
       "4                                   OI  Over 21.37            1     1   \n",
       "5                                  PGNX  Over 3.04            1     2   \n",
       "6  AAP - user if so then the current downtrend wi...         -1     1   \n",
       "7  Monday's relative weakness. NYX WIN TIE TAP IC...         -1     3   \n",
       "8  GOOG - ower trend line channel test & volume s...          1     1   \n",
       "9             AAP will watch tomorrow for ONG entry.          1     3   \n",
       "\n",
       "           Organizations  \n",
       "0                     []  \n",
       "1  [AAP MOVIE, FEA/GEED]  \n",
       "2               [eBooks]  \n",
       "3                     []  \n",
       "4                     []  \n",
       "5                 [PGNX]  \n",
       "6                  [AAP]  \n",
       "7                  [NYX]  \n",
       "8                 [GOOG]  \n",
       "9             [AAP, ONG]  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-island",
   "metadata": {},
   "source": [
    "## Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-guyana",
   "metadata": {},
   "source": [
    "**Exercise 2.0**\n",
    "\n",
    "You have scraped many websites collecting a list of users but written in many different form: someone has an email like:\n",
    "\n",
    "```\n",
    "username: antonio\n",
    "user: antonio.marsella@email.com\n",
    "USER:antonio\n",
    "USERNAME: antonio_\n",
    "```\n",
    "How would you match all of them using a spaCy matcher?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "domestic-footage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:48:55.806530Z",
     "start_time": "2021-03-31T06:48:55.729472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USERNAME 1 4 username: antonio\n",
      "USERNAME 5 8 user: antonio.marsella@email.com\n",
      "USERNAME 9 12 USER:antonio\n",
      "USERNAME 13 16 USERNAME: antonio\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "doc = nlp(\"\"\"\n",
    "username: antonio\n",
    "user: antonio.marsella@email.com\n",
    "USER:antonio\n",
    "USERNAME: antonio_\n",
    "\"\"\")\n",
    "\n",
    "pattern = [{\"LOWER\": {\"IN\":[\"user\",\"username\"]}}, {\"ORTH\": \":\"}, {}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"USERNAME\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-company",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
